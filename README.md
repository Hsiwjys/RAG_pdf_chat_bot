# ğŸ¤– RAG PDF Chatbot (Gemini + Pinecone + LangChain)

This is a Streamlit web app that allows users to upload a PDF document and ask questions about its content. It uses **LangChain**, **Google Gemini Embeddings**, and **Pinecone Vector DB** to build a powerful RAG (Retrieval-Augmented Generation) pipeline.

---

## ğŸš€ Features

- ğŸ“„ Upload any PDF document
- ğŸ” Split content using `RecursiveCharacterTextSplitter`
- ğŸ§  Generate embeddings using **Gemini Embedding (`embedding-001`)**
- ğŸ“¦ Store vectors in **Pinecone**
- ğŸ¯ Ask natural language questions about your PDF
- ğŸ” Corrective RAG: fallback to better answers using retrieved documents
- âœ… View both final answer and source document content

---

## ğŸ“¸ App Preview

![App_UI](images/app_UI.png)

> *(Screenshots placed in `/images` folder â€“ update with your own)*

---

## ğŸ§° Tech Stack

- [Streamlit](https://streamlit.io/)
- [LangChain](https://python.langchain.com/)
- [Gemini Embedding API](https://ai.google.dev/)
- [Pinecone Vector Database](https://www.pinecone.io/)
- [PDF Parsing](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf)

---

## ğŸ§ª How It Works

1. ğŸ“¤ Upload a PDF
2. âœ‚ï¸ Split into text chunks
3. ğŸ§  Embed chunks using Gemini Embeddings
4. ğŸ—ƒï¸ Store in Pinecone (with `namespace`)
5. â“ Ask a question
6. ğŸ¤– LLM tries to answer directly
7. ğŸ›Ÿ If confidence is low â†’ fallback to RAG
8. ğŸ“š RAG returns answer with source references

---

## ğŸ” Streamlit Secrets Configuration

Store API keys safely in `.streamlit/secrets.toml`:

```toml
GOOGLE_API_KEY = "your-gemini-api-key"
PINECONE_API_KEY = "your-pinecone-api-key"
PINECONE_ENV = "your-pinecone-env"
PINECONE_INDEX = "langchainpdf"
PINECONE_NAMESPACE = "llmpdf"
